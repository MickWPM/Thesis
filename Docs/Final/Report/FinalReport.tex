\documentclass[]{aiaa-tc}% insert '[draft]' option to show overfull boxes

\usepackage[toc,page]{appendix} 
 \usepackage{varioref}%  smart page, figure, table, and equation referencing
 \usepackage{wrapfig}%   wrap figures/tables in text (i.e., Di Vinci style)
 \usepackage{threeparttable}% tables with footnotes
 \usepackage{dcolumn}%   decimal-aligned tabular math columns
  \newcolumntype{d}{D{.}{.}{-1}}
 \usepackage{nomencl}%   nomenclature generation via makeindex
  \makeglossary
 \usepackage{subfigure}% subcaptions for subfigures
 \usepackage{subfigmat}% matrices of similar subfigures, aka small mulitples
 \usepackage{fancyvrb}%  extended verbatim environments
  \fvset{fontsize=\footnotesize,xleftmargin=2em}
 \usepackage{lettrine}%  dropped capital letter at beginning of paragraph
% \usepackage[dvips]{dropping}% alternative dropped capital package
% \usepackage[colorlinks]{hyperref}%  hyperlinks [must be loaded after dropping]
%\usepackage{makeidx}


\graphicspath{{Images/}}
%\usepackage[colorlinks]{hyperref}
%\hypersetup{colorlinks = false}
\usepackage{url}

\usepackage{pdfpages}
\usepackage[ampersand]{easylist}

\usepackage{soul}
\usepackage{amsmath}
%\usepackage{subcaption} 
%\usepackage{caption}

\usepackage{natbib}
\bibliographystyle{abbrvnat}
\setcitestyle{authoryear,open={(},close={)}}

%AJL SEIT Comment out these two lines for the final submission
\usepackage{draftwatermark}
\SetWatermarkFontSize{5cm} \SetWatermarkScale{4} \SetWatermarkText{\textbf{DRAFT}}
\pagestyle{plain}

 \title{Towards self driving: Single camera navigation localisation\\ Final report}

 \author{
  Michael McDonnell\thanks{CAPT, School of Engineering and Information Technology, ZEIT4902}\
  \\
  {\normalsize\itshape
   UNSW Canberra at ADFA.}\\
  }

 % Data used by 'handcarry' option
 %\AIAApapernumber{YEAR-NUMBER}
 %\AIAAconference{Conference Name, Date, and Location}
 %\AIAAcopyright{\AIAAcopyrightD{YEAR}}

 % Define commands to assure consistent treatment throughout document
% \newcommand{\eqnref}[1]{(\ref{#1})}
% \newcommand{\class}[1]{\texttt{#1}}
% \newcommand{\package}[1]{\texttt{#1}}
% \newcommand{\file}[1]{\texttt{#1}}
% \newcommand{\BibTeX}{\textsc{Bib}\TeX}

%\makeindex

\begin{document}

\maketitle


\begin{abstract}
20\%: Very well organized structure, highly logical sequence of information, facilitating reader’s understanding, appropriate amount of materials presented in all components

20\%: Major extension of scopes stated in MoU covered, innovative and sound methodology used. AND/OR Very strong justifications are given for the change of scope/methodology used (if any). The changes also greatly improved the project scope and/or the methodology used

20\%: Technical content included in report exceeds normal requirement of AQF8 level, some extensions of knowledge of subject matter created
(D: Strong technical content included in report, full understanding knowledge of subject matter demonstrated, no technical faults found)

20\%: Outcomes achieved well exceed project expectations, excellent conclusion and critical reflection shown. Excellent and in‐depth analysis and deductions are made and good synthesis shown

20\%: Excellent writing that reaches journal paper requirements, mature and professional writing style, excellent/creative used of photos, charts, diagrams etc that greatly enhance the presentation quality, \textbf{concise and excellent literature review }and referencing
\end{abstract}

%\newpage
%\tableofcontents

\newpage
\section{Introduction} \label{sect:intro}
\textbf{TODOTODO:!!! UNDULATION IPM TESTING!!!}

Gen points to cover:
\begin{easylist}
	& Focus on high tech systems - This is a focus on lowest barrier to entry and maintaining a redundancy option
	& Implemented using OpenCV. Used Sim data and confirmed with dashcam data
\end{easylist}

%\index{}
% 
%\lettrine[nindent=0pt]{T}{he} idea of a future where personal transportation is handled by autonomous vehicles is increasingly in the public consciousness however there is a range of challenges that need to be addressed from legal, security and ethical issues \citep{gmReport} to maturity concerns that out to the 2030s `autonomous vehicles will be expensive novelties' \citep{vicTransportImplications}. Autonomous driving is not a binary capability however, rather a scale with increasing levels of autonomy. For context, the automation levels identified by the US National Highway Traffic Safety Administration \citep{automationVisionForSafety} are outlined in table \ref{t:automationLevels}. While it may be decades before true level 5 automation is developed, there is an undeniable increase in the cognitive assistance and partial automation technologies in consumer vehicles. As an example the 2019 Kia Sorento includes active lane keeping assist (lane detection and steering) and adaptive cruise control (autonomous acceleration and braking based off radar distance to leading vehicles) \citep{kia}, which is approaching level 2 automation. 
%
%
%\begin{table}
% \begin{center}
%  \caption{automation levels identified by the US National Highway Traffic Safety Administration \citep{automationVisionForSafety}}
%  \label{t:automationLevels}
%  \begin{tabular}{p{0.1\linewidth}p{0.25\linewidth}p{0.6\linewidth}}
%       Level & Classification & Detail\\\hline
%        0 &  No Automation & Zero autonomy; the driver performs all driving tasks. \\
%       1 &  Driver Assistance & Vehicle is controlled by the driver, but some driving assist features may be included in the vehicle design \\
%       2 &  Partial Automation & Vehicle has combined automated functions, like acceleration and steering, but the driver must remain engaged with the driving task and monitor the environment at all times. \\
%       3 &  Conditional Automation &   Driver is a necessity, but is not required to monitor the environment. The driver must be ready to take control of the vehicle at all times with notice. \\
%      4 &  High Automation &   The vehicle is capable of performing all driving functions under certain conditions. The driver may have the option to control the vehicle. \\
%      5 &   Full Automation &   The vehicle is capable of performing all driving functions under all conditions. The driver may have the option to control the vehicle. 
%  \end{tabular}
% \end{center}
%\end{table}
%
%In order for an autonomous vehicle to navigate effectively there are a few key challenges. The vehicle must have a mechanism to sense the local environment, for example lane detection, as well as the ability to identify and track transient aspects such as other vehicles and on road obstacles. There are many Computer Vision techniques that can assist in providing an understanding of the environment. Direct techniques such as edge and line detection are widely used however Deep Convolutional Neural Networks have also been shown to be effective for road detection \citep{deepRoadSegmentation}.
%
%In addition to the local area, the vehicle must also have the ability to reconcile navigation data with the current location. A supporting concept to this is that of map matching which calculates vehicle location by using the geographical information from sensors such as GPS position, inertial data and map information from a mapping service \citep{keyTechSelfDriving}. Current cutting edge self driving vehicles require high fidelity 3D maps to operate effectively which are time consuming to develop and not adaptive to rapid local changes. Despite this, position localisation improvements have been achieved without high fidelity 3D mapping data using a data fusion of GPS and inertial navigation system data \citep{gpsInsFusion} correlating a detected back lane registry supported with computer vision, GPS and inertial data with map data \citep{lowCostSensorNav} and through use of Kalman filters and LIDAR in more complex environments \citep{robotLIDARSLAM}.
%
%This process of combining data from several sources into a single unified description of a situation is known as data fusion \citep{gpsInsFusion}. Self driving vehicles rely on sensor and data fusion to achieve the four capabilities required of autonomous driving; navigation, path planning, environment perception and car control \citep{keyTechSelfDriving}. This project touches on elements of the first three capabilities in order to localise navigation information. This project will use GPS position data and computer vision techniques to correlate data from preloaded maps to localise localise navigation. The context for this project is general but will loosely use the goal of automating (non-tactical) military field logistics route transport. This will include simplifying assumptions of single lane roads/tracks without the requirement to consider other vehicles or traffic control. The idea of local navigation goals has been explored with the use of Open Street Map data and LIDAR for road mapping with promising success \citep{mitLocalNavDriving} however the ability for a single camera system to facilitate navigation data localisation assists in both system redundancy and lowering the financial and technical barriers to implementation. This project will focus on a `low tech' option to provide a minimum navigation capability that does not rely on more advanced tools such as LIDAR. Consideration of relevant literature will be further outlined in specific sections of current and future work as it is relevant.
%
%In general, the ability to use computer vision to align GPS positioning with mapping service data will allow greater cognitive assistance in driver included tasks without the requirement for significant prior mapping. As augmented reality technology increases this also allows for more immersive driver aides such as navigation routes overlayed onto the visible road. In addition to the cognitive assistance, the ability to localise a navigation route opens the near future possibility for the automation of certain tasks in a military setting, for example routine field logistics supply transport. A true complete solution will rely on sensor fusion from a suit of complimentary sensors supporting each other and providing redundancy. 

Arousal, background/need
This paper reviews a method for navigation localisation and driving line identification for an autonomous vehicle using a single camera system.\textbf{TOOD: After a brief review of related works (\textit{Do we do this in the intro instead??}) a high level overview of the system is given. Subsequent sections investigate discrete elements of the system in order, Road surface detection, route feature matching and route feature tracking. \textit{Additional output options are then briefly covered}. A general discussion including limitation and opportunities follows before concluding remarks.}

\textbf{, scope/intent. }It should be noted that the developed method does not account for road traffic. The use of this method is envisaged to be automation of tasks in remote areas such as logistics in large properties such as farms. Alternatively this method provides a single camera redundancy which may assist the capability to `limp home' in the event of main sensor system failures in more complex autonomous vehicles.

%\textbf{TODO: Terminology?}
%\subsection{Terminology}
%
%\hl{The following abbreviations and definitions are used throughout this report:}
%
%
%\begin{easylist}[itemize]
%	& \textbf{Computer Vision (CV)}. Techniques to allow machines to `see'; processing visual images of the world and deriving understanding.
%	& \textbf{Digital Image Processing (DIP)}. Use of computer algorithms to perform processing on digital images.
%	& \textbf{Navigation localisation}. Translating an overall navigation route into a local navigation goal.
%	& \textbf{Simulation}. The custom built autonomous vehicle simulation which provides sensor feed outputs.
%	& \textbf{External processing}. In the context of this report two programs are discussed, the simulation and `external processing'. External processing refers to the standalone code which performs the computational calculations for navigation localisation.
%	& \textbf{Simulation tick}. One period of simulation processing. This is aligned to a specified update frequency that the external processing is running at `in simulation'.
%	& \textbf{Interprocess Communication (IPC)}. The passage of data from simulation to external processes.
%	& \textbf{Open Street Maps (OSM)}. `A collaborative project to create a free editable map of the world' \citep{osmDataFormat}. Comparable to Google maps.
%	& \textbf{Polyline}. A line defined by node coordinates. The line is drawn through all nodes in order, starting at the first and terminating at the last.
%\end{easylist}

%%
%%\subsection{Project aim}
%%
%%\hl{The aim of this project is to} investigate localising navigation data from a GPS feed to the observed road via a vehicle mounted camera. Mapping service GPS route data is often held as polylines which represents a road as a series of connected points or nodes \citep{googleMapPolyline}, \citep{osmDataFormat}. The approach for positional localisation is to use the vehicle GPS position as an approximate input location mapped to the closest point on a route. Detected road features are then used to determine an accurate position of the vehicle and identify the navigation route on forward facing video feed. This sets the conditions for autonomous control of the vehicle based on a programmed navigation route.
%%
%
%\subsection{Scope and Deliverables}\label{s:scope}
%
%\hl{The scope of the project is deliberately kept constrained initially. This is to focus on the specific problem of localising a navigation route without losing development effort to supporting elements. The scope and deliverables have been identified as follows:}
%\begin{easylist}[itemize]
%	& The solution must be able to reconcile GPS and CV data to identify the current location and required direction to travel through intersections based on a navigation route.
%	& \textbf{Limitation of road complexity.} There is a requirement for road/lane detection as part of this project (to marry up with the GPS polyline data) however optimised road detection is not the main focus of the project. Further as the project purely uses the output data of lane detection, it can be considered a `black box' and implementations can be swapped out as more advanced options are identified. The initial limitations on scope of road detection includes:
%	&& Limit road detection to easily detectable road surface.
%	&& Limit roads to single lane.
%	&& All roads considered will be of similar local colour and type.
%	& \textbf{Simulation deliverable requirements.} In addition to providing data for this project the intent is for the simulation to be held as an asset within SEIT for use in subsequent student projects in this area. The basic requirements for the simulation are:
%	&& Ability to provide 3D video feed of simulated driving to external program.
%	&& Support simulated GPS tracking data.
%	&& Support simulation of GPS route guidance.
%\end{easylist}


\textbf{TODO: In lieu of scope/deliverables etc, include a cover letter in deliverables outlining the additional work done/??}

\section{Related works}
\textbf{TODO: ADD in nav localisation general?}

\citet{ipmForLaneTracking} outlined the effectiveness of Inverse Perspective Mapping for lane detection 

Histogram backprojection has been used for basic \citep{histBackObjectTracking} and multi model object tracking \citep{histBackMultiObjectTrack}, \citep{histBackObjectMultiLighting} , image indexing \citep{histBackImageIndexing} and region of interest detection \citep{histBackObjectOfInterestDetection}. It has been used with sensor fusion including thermal mapping for road detection \citep{histBackThermal} and to effectively refine spatial fuzzy clustering road detection \citep{histBackRefineShadows}.

\citet{moncularLaneDetectAndTrack} outlined an approach involving the use of a Support Vector Machine to segment image road pixels from non-road pixels and an identification of road edges and control points defining the road curve. This approach is somewhat less interpretable however provides a suitable alternate and possibly more robust option for road detection. Additionally the input and output interfaces are the same as in this system which allows the option to `sub in' this approach if it is preferred. The histogram based approach has the theoretical benefit that it is able to adjust to varying road surfaces `on the fly' as opposed to requiring prior training.(*Both systems can be used for redundancy?)\textbf{TODO: Some of this can be extracted to discussion?}

Optical flow LK

??????

%\subsection{Project Methodology}
%
%The initial state of the project was the field of autonomous vehicles as the general area of focus. As a result the preliminary phase of the project was the identification of the `problem area' and narrowing of scope. A broad reading of relevant research and industry articles identified the ability to navigate in arbitrary areas as a candidate problem area and the scope was refined as outlined in section \ref{sect:intro}. 
%
%This project is being undertaken in an area of study that is a new field for the author. As a result the early focus was on developing base competencies in DIP and general CV. This included both an understanding of the theory and mathematics behind DIP and CV tools and familiarity with implementation options and extended into more specific areas such as straight lane detection.
%
%In parallel with the CV competency development, research and experimentation on key technical risk elements of the simulation was conducted. The core technical risk was the ability for simulation code to communicate with sensor processing code. The aim was to keep these two code bases separate to allow other individuals to use the simulation for relevant purposes without being tied in to the aims of this project.
%
%Once the core competencies have been developed and the supporting tool options have been analysed the focus will split between agile development of the simulation and the development of the external processing program. The simulation development will consist of `sprint' periods designed around providing incremental functionality, prioritised as needs arise. The development of the external processing forms the specific primary aim and will include the following core milestones:
%
%\begin{easylist}[itemize]
%	& Lane detection.
%	& Intersection detection and identification of discrete roads.
%	& Mapped road position estimation based off the GPS position and nearby road map nodes (accounting for GPS inaccuracy).
%	& Map matching, specifically some form of curve or spline matching to correlate the estimated map position with the identified road features.
%	& Consolidation of return data. This may include:
%	&& Overlaying the navigation route directly to the video feed.
%	&& Providing control inputs to the vehicle for autonomous driving of the route.
%\end{easylist}
%
%Additional considerations which will be addressed as part of the simulation development include the data structure representation of the GPS maps and navigation routes. These will both be in line with the OSM data structures but may be simplified to specifically the required data for ease of processing in testing. Simplification of data will only occur where it is feasible that the OSM (or similar) data format can be transformed into the used data format with sufficient preprocessing ie. only information that is present in, or able to be derived from, the OSM or similar data will be used.

\section{System overview}
\citep{explainableCNNBookChapter} discusses the importance of explainable and interpretable systems and propose a system to allow interpretability of deep neural perception and control networks. The solution discussed in this paper is developed to be deliberately an inherently interpretable system; at all key stages in the navigation localisation process a human observer can understand intermediate products and the logic of how they were derived. 
More importantly, this allows the entire system to be considered as a modular series. As improved systems, approaches and algorithms are developed, assuming they ahdere to the same input and output data interface, they can be seamlessly swapped in. Additionally the intermediate products are available for use in other systems that may be integrated.

A high level overview of the system is outlined in figure \ref{f:systemOverview}. In general, the system uses inverse perspective mapping to develop a `top down' projection of the vehicle camera image. An averaged histogram of local road pixels is used to develop a probabalistic road surface map using histogram backprojection. An image mask of the upcoming route feature \textbf{(what is a `feature')} is developed using GPS mapping data and is overlaid on the detected road surface. The feature is deemed detected when the proportion of feature mask covered by road surface is above a threshold \textbf{TODO: track these parameters}. Once initially detected, the feature position is updated in subsequent video frames using a mean optical flow between frames and confirmed by remasking. A driving line through the feature can be developed and tracked by using a Bezier curve of the desired route through the feature once the initial feature model has been detected.

The discrete elements of the system with input and output as follows:

\begin{easylist}[itemize]
	& \textbf{Road surface detection}. 
	&& \textbf{Input: }Camera feed. 
	&& \textbf{Output: }`top down' road surface map. 

	
	& \textbf{Route feature matching}. 
	&& \textbf{Input:} `top down' road surface map. 
	&& \textbf{Output: }Position in image of target road feature, if detected.
%	&& Route model mask matching
%	&& 
%	
%	&& \textbf{TODO? Explain - Inverse perspective mapping and camera lens distortion correction}
%	& Probabilistic road surface detection
%	&& \textbf{TODO? Explain - Rolling average histogram for road surface estimation}
%	& Local orientation to road edges
%	&& \textbf{TODO? Explain - Hough transform (IS THIS NEEDED?)}
%	& Route feature matching
%	&& \textbf{TODO? Explain - Matching piecewise model to detected road surface by masking}
%	& Optical flow tracking
%	&& \textbf{TODO? Explain - Using image `flow' to track feature positions for subsequent frame matching and cornering}
%	& Driving path curve matching
%	&& \textbf{TODO? Explain - Bezier curves based off feature points}
	& \textbf{Route feature tracking}. 
	&& \textbf{Input: }Detected location of target route feature and relevant image. 
	&& \textbf{Output: }Updated location of target route feature in subsequent image frame.
\end{easylist}

\begin{figure}
	\centering
	\includegraphics[width=0.95\textwidth]{systemOverview.png}
	\caption{High level overview of system stages, inputs and outputs.}
	\label{f:systemOverview}
\end{figure}
\textbf{TODO: Examples in this paper will consider a the approach to a T junction in simulation}

\section{Road Surface Detection}

In order to localise navigation elements a reasonable estimate of the current road surface is required. This is achieved via Inverse Perspective Mapping (IPM) and histogram backprojection. These aspects are discussed in detail in the following subsections.

\subsection{Inverse perspective mapping}

Inverse perspective mapping is a well established technique that involves a matrix transformation operation to remap pixels from a camera perspective to a `top down' view \citep{compVisionTextbook}. An example of the technique applied to an image from the simulation used is included as figure \ref{f:ipmSim}. Inverse perspective mapping assumes the perspective image is from a flat plane \citep{ipmForLaneTracking} and while this is not the case in route traversal, in most circumstances the road plane can be considered approximately linear about a vehicle position. Further the system developed is robust enough to tolerate introduced error by reasonable undulation. \citep{extendedIPM} have discussed an inverse perspective mapping approach that removes the flat plane assumption however it relies on stereo camera so is out of scope for this system. This approach may be more resiliant and should be considered in the event a stereo camera system is being developed.


\begin{figure}
	\centering
	\includegraphics[width=0.95\textwidth]{RoadDetection/ipmSim.png}
	\caption{Example image from simulation (left) with inverse perspective mapping applied (center) and derived IPM mask (right).}
	\label{f:ipmSim}
\end{figure}

It can be noted that the inverse perspective mapped image has a `null' area in the bottom edges due to the perspective shift. This is used to develop an `IPM mask' which is used in later stages to ensure that any feature matching attempts are not penalised by the zero values in this area as discussed in section \ref{sect:route_feature_matching}. \textbf{TODO: MAKE SURE THIS IS EXPANDED ON IN THE REFERENCED SECTION}

\textbf{TODO: Wider camera FOV helps detection on near approach (better section than this possible though?)}

\textbf{TODO: Maths??}

\subsection{Probabilistic road surface detection}

While the road detection approach considered can be used with both the inverse perspective mapped image and the raw camera perspective image, in this instance it was applied after IPM had been conducted. \textbf{TODO: IS IT BETTER BEFORE? IS IT BETTER AFTER!!?}

Initially a road surface area of interest is defined, based off the near portion of road surface from the vehicle front. While this relies on the assumption that the vehicle starts on a road surface, it has the benefit of flexibility in detecting dynamic road surfaces. \textbf{TODO: TEST THIS.} A histogram of this area is used to inform a rolling average histogram \textbf{TODO: ?? Or a general map?}. This is histogram is backprojected over the image; a probability is determined for each pixel in the image to belong to the defined histogram. An example of the detected road surface from a live camera feed \textbf{TODO: Perspective or IPM???} is included as figure \ref{f:histRoadLive}


\textbf{TODO: Histogram backprojection (SIMULATION AND/OR LIVE IMAGE EXMAPLES, LIVE OF VARYING ROAD TYPES)}
\textbf{TODO: Thresholding vs raw probabilities??? Raw probabilities with `upgraded probabilities based on thresholding???}

\textbf{TODO: Refer to filter effects. 7x7 used for live. 5x5 for sim}

%\begin{wrapfigure}{h}{0.5\textwidth} %this figure will be at the right
%	\centering
%	\includegraphics[width=0.5\textwidth]{RoadDetection/histRoadLive.png}
%	\caption{Detected road surface from live camera.}
%	\label{f:histRoadLive}
%\end{wrapfigure}

\begin{figure}
	\includegraphics[width=0.95\textwidth]{RoadDetection/histRoadLive.png}
	\caption{Detected road surface from inverse perspective mapped live camera.}
	\label{f:histRoadLive}
\end{figure}

\citet{histBackRefineShadows} outline a shortfall with this style of road surface detection is that it can be prone to error when the road surface has little colour difference to the surrounding environment. The approach suggested in that case includes the use of thermal sensing so is not applicable for this system. If the system is required to operate in a more difficult to segment environment a more robust road detection approach may be needed. 


\section{Route Feature Matching}\label{sect:route_feature_matching}

The system matches route `features' based on points defining relevant road segments. While GPS mapping data format can vary the general common factor is roads defined by a series of points. These points can be used to develop a model of the road and in this case, key features. Features considered involved road intersections such as T junctions and side roads. A model of the feature was developed by linear interpolation between defined points
\textbf{TODO: LOOK FOR REFS HERE?}
\subsection{Feature mask development}

In order to develop a feature model, the feature point (node central to the intersection in this case) was placed as a starting point on the middle of a blank mask. The feature mask is then developed by linearly connecting adjoining nodes \textbf{TODO: Width is hyperparameter}. It is important to not at this stage that the feature mask consists of multiple sub masks; each connection to the feature point is kept as an independent mask. The width of these connections is scaled to the desired projected width for vehicle movement. \textbf{TODO: Once the mask is developed it is then rotated so the approach line is vertical at the base of the image; this ensures the feature mask orientation will match the detected road orientation.} An overview of the feature mask creation is included as figure \ref{f:featureMaskDevelopment}. The final step in developing the feature mask is to relate it to the non-null areas of the inverse perspective mapped image. This is done using a bitwise AND operation between the developed feature mask and the inverse perspective map mask. The purpose of this step is to ensure that portions of mask features such as side streets are not considered if they fall outside the inverse projected image space. \textbf{TODO: FIGURE EXPLAINING THIS}

\begin{figure}
	\centering
	\includegraphics[width=1\textwidth]{FeatureMatching/featureMaskDevelopment.png}
	\caption{Process of developing feature mask developed feature nodes. Nodes are placed (left) and individually connected to inbound and outbound nodes (center left). A resulting mask is combined with the IPM mask (null area shown as TODOODODODO THIS) (center right) to obtain the final model mask (right)}
	\label{f:featureMaskDevelopment}
\end{figure}

\subsection{Feature mask matching} 

Let $P(sub,k)$ be the probability that the k'th sub feature $\textbf{F}_{sub,k}$ is detected and $P(feature)$ be the probability that a feature $\textbf{F}_{route}$ is detected for a given detected road surface $\textbf{R}_{detected}$. $P(sub,k)$ is determined by the summation of the Hadamard product between the feature sub mask and the detected road surface divided by the element sum of the feature sub mask, as outlined in equation \ref{eq:subMaskProbability}. The assessed probability that the route feature is detected is the minimum sub feature probability as per equation \ref{eq:featureProbability}. \textbf{TODO: The probability threshold for $P(feature)$ is a design decision that can be amended based on factors such as noise and road surface detection output (for example probabilistic or binary thresholded).}

\begin{equation}\label{eq:subMaskProbability}
	P(sub,k) = \frac{\sum_{i,j=1}^{n} (\textbf{R}_{detected} \circ \textbf{F}_{sub,k})_{ij}}{\sum_{i,j=1}^{n} \textbf{F}_{sub,k,ij}}
\end{equation}

\begin{equation}\label{eq:featureProbability}
	P(feature) = min\{\textbf{F}_{sub,k}:k=1,...,n\}
\end{equation}

Initial detection approaches involved using a single full feature mask as per equation \ref{eq:subMaskProbability} in lieu of sub features however this approach is significantly less robust. \textbf{TODO: WHY IS SINGLE MASK BAD}

Once the feature is detected, the center point is then the main feature node location from the initial mask. \textbf{TODO: It is possible to further refine this by `bracketing' either side however assuming the vehicle is central on the road surface the midpoint of the detected feature will be aligned to the midpoint of the route feature.}


\section{Road Feature Tracking}

In order to track the detected route features mean optical flow was used to estimate an updated feature node location which was then confirmed using the masking approach discussed in section \ref{sect:route_feature_matching}. In general optical flow options can be considered as sparse where a few key points are tracked, or dense which tracks a large number of points, up to individal pixels (although it should be noted this categorisation represents ends of a continuous range rather than a binary category). 
\textbf{TODO: Once the feature is tracked???}

\subsection{Optical flow tracking}

Sparse options \textbf{TODO: eg?} were initially considered for efficiency however the two frame estimation proposed by \citet{opticalFlowSolution} (the `Gunnar Farneb{\"a}ck method') was identified as the prefferred optical tracking approach. The Pyramidal Implementation of the Lucas Kanade Feature Tracker outlined by \citet{opticalFlowLKPyramidal} relies on key tracking points in an image and it is assessed that the random surface textures in this case lack the definition to be tracked effectively via this method. Similar results were identified by \citet{opticalFlowLKvsDenseUAV} when considering moving images of near grass surfaces. The optical flow tracking implemention involved tracking a small subsection of interest from the inverse perspective mapped image converted to greyscale. The Gunnar Farneb{\"a}ck method considers a polynomial expansion to approximate pixel neighbourhood and minimises an error function for an approximated local displacement and performed well in this situation. \textbf{TODO: Results comparing the two? Maybe }

The benefits of optical flow tracking in a smaller region of interest of the inverse perspective map are as follows:
\begin{easylist}
	& Allows effective processing of dense optical flow due to a small total number of pixels.
	& Avoids noisy areas such as extremity pixels which are warped by inverse perspective mapping.
	& Optical flow space is in the same space as the feature matching allowing a direct application of mean flow to the new estimated position.
\end{easylist}

A visual example of optical flow output for a small selection of pixels is included as figure \ref{f:optical_flow_trails}. It can be seen from this image that not all flow vectors are uniform and in particular at the edges of the mask there are large distortions. These distortions are expected due to the discontinuity and do not affect the result as the system only considers the optical flow from a range of pixels to the direct front of the vehicle. The average of these more uniform flow vectors is used to provide an effective estimate of the updated feature location.


\begin{wrapfigure}{h}{0.5\textwidth} %this figure will be at the right
	\centering
	\includegraphics[width=0.5\textwidth]{FeatureTracking/optical_flow_trails.png}
	\caption{Selected optical flow lines visualised. Black tails indicate flow vector from each point.}
	\label{f:optical_flow_trails}
\end{wrapfigure}

\textbf{TODO: Frame by frame tracking - UPDATE FEATURE ORIENTATION AT NODES}
\textbf{TODO: Confirm using route feature matching as per section \ref{sect:route_feature_matching} }


\textbf{APPLY IPM MASK TO SHIFTED MASKS}

\subsection{Driving path curve matching}

\textbf{TODO: Bezier curve basics}

\textbf{TODO: Develop curve based on inbound, central and outbound node}

\textbf{TODO: Add to intersection mask to ensure driving line on detected road}



The real challenge in this problem is that most route features require more than a simple maintenance of central position such as turning at an intersection. In order to effectively identify a path through a route feature, a driving path is developed using bezier curves.


\textbf{TODO: DO we need to create control points near intersection for p0 and p2 or do the poly line points work ok? I imagine we will need to artificially create these entry/exit points from the feature!!! This can be done simply by moving X meters along the approach/exit line to determine entry/exit control points}

\section{Additional optional outputs}

\subsection{Local road edge orientation}
Edge detection and Hough Transforms to identify main edges. Averaged position represents a general central point to the road which can be used as a standing control input.

Edge detection through Support Vector Machine \cite{moncularLaneDetectAndTrack}

While it was not explicitly considered as a feature, any road feature with a sharp enough angle between points can be used. This is particularly helpful if an aim is to identify a `true' position outside GPS error as sharp bends in a road can be used as additional features.

\section{ Discussion?}
\textbf{TODOTODO:!!! UNDULATION IPM TESTING!!!}

\textbf{TODO: histogram - rolling average, MAX with known road maps?}

simpler approach - just model feature as is in final approach - avoids rotation requirement

\textbf{TODO: Discuss Kalman filtering for position estimation}

update frequency and resolution of images

\textbf{TODO: System designed to work off GPS informed however can use features as `directions'}

\section{Concluding remarks}

%
%\subsubsection{Bezier curve theory}
%\textbf{TODO: ADD THIS AS APPENDIX??!?}
%A Bezier curve is a parametric curve defined by a series of control points with the position on the curve defined by a variable $t$ ranging between 0 and 1, corresponding to the start and the end of the curve. Quadratic Bezier curves consist of three control points ($p_0$, $p_1$ and $p_2$) resulting in a quadratic equation with respect to the variable $t$. \textbf{TODO: Keep going}
%
%\textbf{TODO: IS THIS MID POINT INTERPOLATION REQUIRED? OR DOES SETTING `IDEAL' CONTROL POINTS FIX IT?}
%The formula for the basic Quadratic Bezier curve is outlined as equation \ref{eq:quadratic_bezier}. This will result in a curve as per the curve in figure \ref{fig:quadratic_bezier_simple}. A shortfall with this approach is it will result in `cutting' of the corner as the point $p_1$ is the center point of the feature. This can be addressed by \hl{using what was $p_1$ as the central point $p_c$ to calculate a middle control point to force the curve through this point}. If a constraint is added that $\textbf{B}(t)=p_c$ for some value of $t$, it is then possible to solve for the required $p_1$ to fit this constraint. The free parameter in this instance is the value of $t$ where the curve passes through $p_c$. Setting $t=0.5$ and solving for $p_c$ will set the curve to pass through $p_c$ at the mid point which was deemed suitable in this instance. The equation to force the $p_c$ constraint is included as equation \ref{eq:quadratic_bezier_constraint} and the result of forcing the constraint  $\textbf{B}(0.5)=p_c$ is visualised in figure \ref{fig:quadratic_bezier_center_point}.
%
%
%\begin{figure}[!tbp]
%	\centering
%	\begin{minipage}[b]{0.4\textwidth}
%		\includegraphics[width=\textwidth]{bezier/quadratic_bezier_curve.png}
%		\caption{Quadratic Bezier curve from control points ($p_0$, $p_1$ and $p_2$)}
%		\label{fig:quadratic_bezier_simple}
%	\end{minipage}
%	\hfill
%	\begin{minipage}[b]{0.45\textwidth}
%		\includegraphics[width=\textwidth]{bezier/quadratic_bezier_curve_central_point.png}
%		\caption{Quadratic Bezier curve from control points ($p_0$, $p_1$ and $p_2$) constrained to pass through $p_c$}
%		\label{fig:quadratic_bezier_center_point}
%	\end{minipage}
%\end{figure}
%
%
%\begin{align}
%\textbf{B}(t) &= (1-t)^2p_0 + 2t(1-t)p_1 + t^2p_2 \label{eq:quadratic_bezier} \\
%p_1 &= (p_c - (1-t)^2p_0 - t^2p_2)/(2t(1-t)) \label{eq:quadratic_bezier_constraint} \\
%p_{mid} &= (1-a)p_1 + ap_c \label{eq:interpolated_p1}
%\end{align}
%
%It is clear the developed both path curves in both figures \ref{fig:quadratic_bezier_simple} and \ref{fig:quadratic_bezier_center_point} are not an ideal solution. As a result the final approach was to use a weighted interpolation between $p_c$ and $p_1$ where $a$ is a weighting between 0 and 1 for $p_c$ as the middle control point position; when $a=0$, $p_c$ will have full weighting over the calculated $p_1$ as the middle control point. \textbf{TODO: a = 0.5}
%


%\begin{wrapfigure}{R}{0.5\textwidth} %this figure will be at the right
%	\centering
%	\includegraphics[width=0.5\textwidth, height=0.5\textwidth]{early_lane_detection_experiment.png}
%	\caption{Still from video feed lane detection experiment. Lower area of interest where frames are averaged visible in the bottom central part of the image}
%	\label{f:simpleLaneDetectionHough}
%\end{wrapfigure}


%\section{Current work}\label{s:currentWork}
%
%The bulk of the initial effort has been developing technical competency within informing disciplines. This includes both the CV technical competency development and addressing the simulation technical risks. This completed work is tracked in the Gantt chart in appendix REMOVED. Additionally to these aspects, familiarity projects were conducted on neural networks to consolidate the basic theory for potential future use. These `toy' projects consisted of developing neuroevolution based solutions for both a physics based ball balancing challenge over the duration of 5 minutes and custom cloned version of the mobile game `flappy bird'. Both solutions used a simple feedforward neural network coded in C\# without the use of any existing libraries with the networks evolving using a custom built genetic algorithm. The CV and simulation aspects are discussed in more detail in the following subsections. 
%
%\subsection{Computer Vision}\label{s:currentWork_CV}
%
%Before any effective computer vision approaches can be implemented a robust base of understanding of DIP is required. In order to develop a robust understanding of DIP concepts study was undertaken using a range of publicly available resources. The main resource used for structured learning was the Spring 2015 offering of ECSE-4540 at Rensselaer Polytechnic Institute in New York by Rich Radke which has the full set of recorded lectures freely available online. Basic algorithms were implemented from first principles in MATLAB which reinforced the internalisation of the concepts. More advanced computer vision concepts were subsequently researched and tested within Python using OpenCV functionality which is discussed further in this section. 
%
%The Hough Transform was used to detect straight lanes with varying levels of success. The Hough Transform converts a global detection problem into a local peak detection problem in a parameter space \citep{surveyOfHT} where straight lines in an image can be represented by a single point in Hough space \citep{houghPaper}. A brief overview of simple lane detection as a conceptual primer for the interested reader, including a discussion on the Hough transform, is included as appendix REMOVED
%
%\begin{wrapfigure}{R}{0.5\textwidth} %this figure will be at the right
%	\centering
%	\includegraphics[width=0.5\textwidth, height=0.5\textwidth]{early_lane_detection_experiment.png}
%	\caption{Still from video feed lane detection experiment. Lower area of interest where frames are averaged visible in the bottom central part of the image}
%	\label{f:simpleLaneDetectionHough}
%\end{wrapfigure}


%
%It was noted that the simple approaches suffered when the dashed lane line was faint in comparison with other linear features and when the road was cast in intermittent shadow. The final experiment to improve on detection involved lane detection on a video feed using a rolling average of a lower area of interest of the most recent 5 frames of video as input to the lane detection for each frame. The lane detection approach involved identifying straight lines using the Hough Transform of the Canny edge detected area of interest after a low pass filter had been applied. The Hough lines were then chosen based on strength with the strongest line for each angle (side of the lane) chosen. The output consisted of the averaged area of interest with the detected lane lines in red overlayed onto the original frame. An example frame from the video output is included as figure \ref{f:simpleLaneDetectionHough}. 
%
%In order to identify the true shape of the road, more advanced techniques are required. Inverse perspective mapping is `a geometrical transform that attempts to remove the perspective effect present in an image captured by a camera pitched down towards a road' \citep{intersectionDetectionSingleCamera}, allowing a birds eye view of the scene by weighing each pixel according to its information content \citep{stereoIPM}. Inverse perspective mapping has many uses within the autonomous driving realm including vanishing point detection  \citep{ipmVanishingPoint}, attitude noise reduction \citep{ipmAttitudeNoise}, distance determination \citep{ipmDistanceDetermination}, regularizing optical flow \citep{ipmOpticalFlow}, improving relative velocity determination \citep{ipmOpticalFlowSpeed} and detailed depth information in stereo \citep{stereoIPM}. Importantly though, inverse perspective mapping is a key part in effective lane and obstacle detection \citep{ipmOpticalFlow} \citep{ipmDistanceDetermination} \citep{ipmBasedLaneDetectionApproach} \citep{ipmVanishingPoint}.
%
%The basic approach to lane detection using CV then involves correcting the image for camera lenses before inverse perspective mapping to get a `birds eye view' of the road. From this top down perspective, road/lane analysis can be conducted properly. An inverse perspective mapping calibration for the simulation was developed using the IPC implementation discussed in section \ref{s:currentWork}-\ref{s:simSystem}-\ref{s:IPC}. This calibration consists of still image of a checkerboard pattern on the horizontal plane within the simulation taken from the perspective of the camera. Corners of the checkerboard are manually identified in pixel space and the inverse perspective matrix is calculated based on the current old (perspective) and desired new (inverse perspective) positions. Automated identification of checkerboard corners is possible however the simulation camera rotation with reference to the horizontal plane is fixed therefore only a single calibration is required. The input and output of the inverse perspective mapping calibration undertaken is included as figure \ref{f:inverse_perspective_calibration}. Key considerations within the Computer Vision implementation decision process are in the following subsections.
%
%\begin{figure}[htb]% order of placement preference: here, top, bottom
%	\includegraphics{InversePerspectiveEg.png}
%	\caption{Inverse perspective mapping calibration. Original camera feed \textit{(left)} and inverse perspective mapped image \textit{(right)}}
%	\label{f:inverse_perspective_calibration}
%\end{figure}
%
%\subsubsection{Use of OpenCV}\label{s:openCV}
%
%The DIP theory that was undertaken opened up the possibility to implement the CV functionality from first principles, indeed basic tools such as filtering had already been implemented in MATLAB experiments. The alternative option considered was to use an open source computer vision library called OpenCV. OpenCV has C++, Python, Java and MATLAB interfaces and is widely used in commercial applications in companies such as Google, IBM and Honda \citep{opencvWebsite}. While implementing all functions manually has merit for further reinforcement of the basic principles, the decision was made to use the OpenCV library. Rationale for this includes the following: 
%
%\begin{easylist}[itemize]
%	& Efficient high performance vectorised implementations of CV algorithms.
%	& Allows a greater proportion of time to be spent on solving the problem as opposed to implementing known algorithms.
%	& Increase familiarity with industry level tools for future use.
%\end{easylist}
%
%\subsubsection{Language choice} \label{s:pythonVc}
%
%Given the choice of OpenCV for the CV framework the options considered for the core language were C++ or Python. C++ has the advantage of being `lower level' however the OpenCV Python interface compiles down to the same C++ code so the only performance gains would be in the general program code as opposed to the CV implementations. Simulation engine considerations also played a part, as discussed in section \ref{s:currentWork}-\ref{s:simSystem}-\ref{s:simEngineChoice}, Unreal Engine uses C++ which would allow simulation code to talk directly to the OpenCV interface.
%
%Despite the above advantages, C++ comes with an increased complexity and any speed gains require a robust understanding (and implementation) of memory management. By contrast, Python presents a lower barrier to entry while maintaining the OpenCV C++ vectorised optimisation. In addition, libraries such as numpy within Python present additional highly optimised operations for array data manipulation. The decision to use Unity for the simulation engine, as discussed in section \ref{s:currentWork}-\ref{s:simSystem}-\ref{s:simEngineChoice}, removed the major benefit of C++, being the potential for the simulation code to talk directly to OpenCV, thus Python was determined to be the preferred language.
%
%\subsection{Simulation system}\label{s:simSystem}
%
%A significant element of the project deliverable is the development of the simulation system to provide sensor outputs. The intent of the simulation is to provide a pipeline of sensor feeds and potentially an interface to control a simulated vehicle based off the processed sensor data. The scope of the simulation is discussed in section \ref{sect:intro}-\ref{s:scope} with key elements of the simulation system discussed in the following subsections.
%
%
%\subsubsection{Simulation engine} \label{s:simEngineChoice}
%
%Two courses of action were available for the implementation of the simulation engine; development of a full custom simulation engine or implement simulation logic within an existing game engine. For the existing game engine option, Unreal and Unity were both considered. A full custom engine allows fine grain control over the implementation the specific requirements of the simulation however has significant overheads including the implementation of 3D visuals including rendering, texturing and importing of 3D models. It was determined that this represented a very large time commitment for negligible benefit.
%
%Both Unity and Unreal are professional 3D engines used in commercial games and computer graphics applications. They are well established and have a robust set of supporting tools. Specific considerations for each engine are as follows:
%
%\begin{easylist}[itemize]
%	& Unity:
%	&& Personal experience with workflow and language (C\#). Comfortable implementing desired features.
%	&& No C\# interface with OpenCV therefore requirement for IPC to be developed between the OpenCV processing implementation and the simulation engine.
%	& Unreal:
%	&& New, unfamiliar workflow.
%	&& Lack of familiarity and comfort with C++ which is used for scripting within the engine. `Blueprint' system does allow a visual programming approach although that represents an additional competency to be developed.
%	&& C++ code allows direct interface with OpenCV.
%\end{easylist}
%
%When considering the above factors and the OpenCV considerations discussed in section \ref{s:currentWork}-\ref{s:currentWork_CV}-\ref{s:openCV}, Unity was chosen due to the ability to `hit the ground running' based on workflow and language familiarity. It was noted that this accepts a large technical risk of IPC implementation however it was deemed to be the preferred option.
%
%\subsubsection{Interprocess Communication (IPC)} \label{s:IPC}
%
%One of the main issues identified with using Python Open CV and the Unity game engine is the ability for simulation data, for example video feeds, to be transferred from Unity to the Python process. This was assessed as a significant technical challenge and is the single largest time budgeted feature in the initial simulation work. The motivation for fast IPC was to allow real time processing of the simulation data in order to allow for potential control feedback to the simulation. Transferring image data represents a large amount of memory thus has a significant time complexity per simulation `tick'. 
%
%The initial research approach involved investigating options such as static memory buffers, shared memory and memory mapped files which would allow direct memory access to data. These options added another level of complexity and it was determined that a lower technical risk solution was to use TCP via the ZeroMQ middleware. ZeroMQ is a communications interface with implementations for both C\# and Python and is well optimised for speed.
%
% The use of this approach potentially results in slower than real time simulation processing which was mitigated by dynamic simulation time dilation, discussed in section \ref{sect:timedilation}. This approach allows easy and reliable two way communication which is required for effective feedback between the processes. This solution also offers the ability to expand off a single local machine, for example a hosted simulation with remote processing. While this is well out of scope of this research it presents additional flexibility for future development.
%
%Once the IPC approach was confirmed, the data handling pipeline between the Unity (C\#) and Python processes was developed. The final simulation output data structure for video feed is a flattened 3D byte array with four bytes for each $(x,y)$ pixel coordinate, representing the Red, Blue, Green and Alpha image colour channels. The Python process casts the byte stream as a byte array and reshapes the data to a 3D array. The fourth element of each colour value (the Alpha channel) is redundant and is removed. This results in a final data structure consisting of a 2D array of 3 element vectors of bytes which is the required data structure for OpenCV processing operations. 
%
%\subsubsection{Simulation time dilation}\label{sect:timedilation}
%
%There is a requirement to implement dynamic time scaling in the simulation order to maintain time synchronisation with the external process. The simulation scales time based on the desired processing update rate and the real time that the IPC process and external data processing takes. This allows simulation complexity scaling without the danger of temporal desynchronisation between simulation and external processes.
%
%The implementation of time dilation involved a simulation controller that runs the simulation for a set time based off the desired sensor processing rate. After this time has elapsed, sensor data is collated and sent to the external processing program via the IPC process outlined above and the simulation pauses. On completion of the data processing, the external program sends an acknowledgement in return which triggers the next simulation tick. This has been implemented and was initially tested by sleeping the processing thread for several seconds each tick. Time dilation with IPC was also tested using an initial prototype of the simulation; two stills from this test from the Python process output video showing the raw feed and the canny edges is included as figure \ref{f:simPrototypeIPCTest}.
%
%\begin{figure}[htb]% order of placement preference: here, top, bottom
% \includegraphics{simPrototypeIPC.png}
% \caption{Still from video feed of simulation prototype test. Original camera feed \textit{(left)} and canny edges of feed \textit{(right)}}
% \label{f:simPrototypeIPCTest}
%\end{figure}
%
%\section{Future work}
%
%
%\subsection{Computer Vision}
%
%\subsubsection{Lane detection}
%
%Detection of straight lane lines has been completed in the familiarisation activities conducted as discussed in section \ref{s:currentWork}-\ref{s:currentWork_CV} and demonstrated in figure \ref{f:simpleLaneDetectionHough}. Alternative approaches to lane detection which may be considered include k-means clustering \citep{ipmBasedLaneDetectionApproach}, road classification from histogram based segmentation \citep{histogramSegmentationRoadClassification}, road surface mapping from near field driving surface models \citep{darpaChallengeRoadDetection}, matching road curvature models to detected lane boundaries \citep{intersectionDetectionSingleCamera} and open uniform B-spline model lane fitting \citep{ipmBasedLaneDetectionApproach}.
%
%A more complex challenge is identifying a curved lane. Supervised convolutional neural network lane detection has had success through fully convolutional \citep{cnnLanes1} and instance segmented \citep{cnnLanes2} approaches. Spline based representation using random sample consensus for bezier splines based off road edge detection \citep{ransicBezierFit} has also shown to be effective. The current planned approach is lane edge detection via polynomial fitting of edge pixels detected by sliding window. An overview of the current implementation plan for the sliding window detection approach is included as appendix REMOVED. The technical risk of this aspect is low to medium.
%
%
%\subsubsection{Intersection detection}
%
%Intersection detection presents a more challenging prospect than simple lane detection. The simplifying assumption of a single left and right lane edge is removed when intersections are introduced. The SCARF (Supervised Classification Applied to Road Following) approach of using Gaussian colour models to identify road surface before road model matching \citep{scarf} and UNSCARF (UnSupervised Classification Applied to Road Following) approach using feature identification, clustering and edge detection and road model matching \citep{unscarf} have both demonstrated an ability to detect lanes and intersections in non ideal conditions using a dual camera setup, although not in real time \citep{scarfAndUnscarfPresented}. A data driven model using a `virtual camera' supported by model based intersection matching has also shown promise \citep{alvinnVC} however this requires prior information to reconstruct virtual images. Alternatively, once a binary road surface map is obtained, row based histograms can be used to identify intersection locations by interpolation between intersection models \citep{intersectionDetectionSingleCamera}.
%
%One example of effective intersection detection determined to be more directly applicable to this project is model based recognition which matches intersection models to a series of road boundary points \citep{modelBasedIntersection}. An alternate approach is using skeletonisation of the road binary mask. When the full road surface is represented by a binary mask, skeletonisation will reduce each road to a single pixel line. This road skeleton can be matched to the model skeletons. The current intent is to use edge detection to determine boundary points with the intersection model being built reasonably simply using the OSM map data. The technical risk of this aspect is medium.
%
%
%\subsection{Navigation localisation}
%
%The final step involves correlating the GPS positional data, features from video sensor feed and the navigation data to localise the navigation data to the vehicle position. Static route map matching at its most trivial relates a GPS position to the nearest point on a polyline. It is anticipated that this will be the first step in the localisation to approximate the position on the navigation route. 
%
%A LiDAR-based local trajectory generation has demonstrated through practical experiments that effective localisation using OSM data and global waypoints is possible using a LIDAR sensor suite \cite{mitLocalNavDriving}. General curve or spline matching options will be investigated and it has been identified that Markov and extended Kalman filter localisation techniques may also assist in this problem \citep{probabalisticRobotics}. The most likely solution will be a combination of the above and the model based recognition outlined by \citet{modelBasedIntersection}. The technical risk of this aspect is medium.
%
%\subsection{Simulation}
%
%The simulation development will follow the agile development process and deliver incremental feature additions and improvements. The basic features which are high priority for the early sprints are the implementation of the functionality required for the minimum viable product; the vehicle controller, the road network and the GPS sensor and navigation implementations. These aspects, as well as candidate functionality extensions are discussed in more detail in the coming subsections.
%
%\subsubsection{Vehicle controller}
%
%The vehicle controller will be implemented using the built in physics components within the Unity engine. The basic vehicle setup will consist of steering, accelerating and braking functions with vehicle movement simulated by the physics engine. An autopilot function that will follow defined waypoints from the navigation system discussed in section \ref{s:gpsNavSim} will also be developed. It is important to note that this autopilot is not based on the external analysis of sensor input but will use the exact game data. The intent of this functionality is to provide a constant video feed of a driving vehicle by a simulated driver agent for analysis and should not be confused with the autonomy solution to the main problem this research is investigating. Features such as engine power simulation will not be implemented in the initial stages but may be introduced as future features. The technical risk of this aspect is very low.
%
%\subsubsection{Road network}
%
%The road network is being implemented using a commercial tool called Easyroads 3D. This tool assists in mesh generation, placement, connection and texturing of roads as well as terrain moulding. Additionally the tool can accept map data exported from OSM. While each of these functions can be custom built individually it represents a significant time commitment and is unrelated to the core simulation. The road visible in the raw video feed of the IPC prototype test in figure \ref{f:simPrototypeIPCTest} demonstrates the implementation of a test road in Unity. The initial implementation of the road network will be simple single lane tracks. The technical risk of this aspect is low.
%
%\subsubsection{GPS sensor}
%
%The GPS sensor implementation is a simple function and required for the base sensor data analysis. The functionality will use the in engine coordinates for the GPS coordinates, as will all other data and sensors. An initial customisable random position offset will be used to simulate GPS positional error however additional functionality improvements may include a higher fidelity simulation of GPS error. The technical risk of this aspect is negligible.
%
%\subsubsection{GPS navigation}\label{s:gpsNavSim}
%
%The navigation functionality will initially be via hardcoded hand placed waypoints, using a simple queue system, with waypoint positions using the in engine coordinate system. Waypoints will be dequeued from a distance threshold based on GPS position proximity to the next waypoint. This represents the minimum viable product. There is significant room for expansion of functionality here including the ability to automatically route plan based off the OSM data used to build the road navigation network which can be queried using a path algorithm, most likely A*. This aspect has significant room for scope creep so will be managed tightly in the sprint reviews. The technical risk of this aspect is low to medium.
%
%\subsubsection{Possible Extensions}
%
%The implementation of further functionality is based on the sprint velocity achievable and the needs at the time. Once the minimum viable product with the above functionality is delivered it is likely that improvements to the existing functionality will take a high priority in future sprints however additional functionality will be considered in the sprint backlogs. Possible future extension ideas include:
%\begin{easylist}[itemize]
%	& External sensor definition files for automatic setup in simulation.
%	& External OSM data file loading. Implementing this will also result in the requirement for automated data extraction from OSM files for simulated GPS and navigation.
%	& Increased communication options between simulation and external process, for example vehicle control signals or specific sensor control/activation/deactivation.
%\end{easylist}
%
%\section{Conclusions}
%
%The main technical risks for the sensor data processing include the detection of curved roads and intersections and the navigation localisation. Of these risks, the intersection detection is the most significant risk as it has the burden of providing an output that can be effectively used by the navigation localisation step. The main technical risks for the simulation have been successfully addressed and the development of the simulation is ready to enter the agile development process. This will deliver iterative improvements in functionality and provide a wealth of sensor data for analysis testing. Despite the identified risks, the research conducted has identified candidate solutions for each area of technical risk and rapid iteration through the areas of technical risk will assist in assessing the relative merit of the identified approaches.





%\section{Recommendations}
%This section should discuss and recommend directions for future work that will build on and extend your research and perhaps resolve some of the issues that you have encountered in your work.
%
%\section*{Acknowledgements}
%The Acknowledgements section should be used to briefly thank those individuals or organisations that have assisted you directly in your thesis work whether they be family, friends and colleagues, or technical and academic staff. Note that any external funding source that supported your project should be acknowledged here. 
%

%\newpage
%\section*{Appendices}
%\ref{app:ganttChart} - Gantt chart \\

%\ref{app:briefOverviewOfLaneDetection} - A brief overview of a simple approach to lane detection \\

%\ref{app:slidingWindow} - Sliding window detection for curved roads \\


% AJL - UNCOMMENT THIS IS PREFERENCE TO THE ABOVE SECTION
%% produces the bibliography section when processed by BibTeX
\bibliography{References}
%\bibliographystyle{aiaa}


%\appendix
%\pagestyle{empty}
%\includepdf[angle=-90,scale=1,pages=1,pagecommand=\section{Gantt Chart for Research Project}\label{app:ganttChart}]{GanttChart.pdf} 

%\includepdf[pages=1,scale=.8,pagecommand={\section{A brief overview of a simple approach to lane detection}\label{app:briefOverviewOfLaneDetection}},linktodoc=false]{briefOverviewOfLaneDetection.pdf}
%\includepdf[pages=2-,scale=.8,pagecommand={},linktodoc=false]{briefOverviewOfLaneDetection.pdf}

%\includepdf[angle=-90,scale=0.9,pages=1,pagecommand=\section{Sliding window detection for curved roads}\label{app:slidingWindow}]{curvedLaneDetection.pdf} 

\end{document}

