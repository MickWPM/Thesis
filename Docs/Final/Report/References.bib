% Encoding: UTF-8

@InProceedings{moncularLaneDetectAndTrack,
  author    = {N. {Mechat} and N. {Saadia} and N. K. {M'Sirdi} and N. {Djelal}},
  title     = {Lane detection and tracking by monocular vision system in road vehicle},
  booktitle = {2012 5th International Congress on Image and Signal Processing},
  year      = {2012},
  pages     = {1276-1282},
  month     = {Oct},
  doi       = {10.1109/CISP.2012.6469683},
  keywords  = {CCD image sensors;image classification;Kalman filters;object detection;object tracking;road vehicles;splines (mathematics);support vector machines;traffic engineering computing;road scene image;tracking algorithm;Kalman filter;lane model;Catmull Rom spline;SVM classifier;support vector machine;lane detection algorithm;road environment;autonomous vehicle control;driving assistance;single camera CCD;lane tracking system;road vehicle;monocular vision system;Roads;Splines (mathematics);Support vector machines;Read only memory;Vehicles;Mathematical model;Image segmentation;Lane detection;Lane tracking;Support Vector Machine;Catmull Rom splines;Machine vision;Kalman filter},
}

@InProceedings{ipmForLaneTracking,
  author    = {A. M. {Muad} and A. {Hussain} and S. A. {Samad} and M. M. {Mustaffa} and B. Y. {Majlis}},
  title     = {Implementation of inverse perspective mapping algorithm for the development of an automatic lane tracking system},
  booktitle = {2004 IEEE Region 10 Conference TENCON 2004.},
  year      = {2004},
  volume    = {A},
  pages     = {207-210 Vol. 1},
  month     = {Nov},
  doi       = {10.1109/TENCON.2004.1414393},
  keywords  = {roads;tracking;road vehicles;image representation;traffic engineering computing;inverse perspective mapping algorithm;automatic lane tracking system;vision based automatic lane tracking system;road curvature;road image;preprocessing stage;transformation technique;foreshortening factor;vanishing point;Cameras;Road accidents;Equations;Automotive engineering;Road vehicles;Vehicle detection;Systems engineering and theory;Nanoelectronics;Shape;Predistortion},
}

@Book{compVisionTextbook,
  title     = {Computer Vision},
  publisher = {Prentice Hall},
  year      = {1982},
  author    = {Dana H. Ballard and Christopher M. Brown},
  isbn      = {0131653164},
}

@InProceedings{extendedIPM,
  author    = {Bertozzi, Massimo and Broggi, Alberto and Fascioli, Alessandra},
  title     = {An extension to the Inverse Perspective Mapping to handle non-flat roads},
  booktitle = {IEEE International Conference on Intelligent Vehicles. Proceedings of the 1998 IEEE International Conference on Intelligent Vehicles},
  year      = {1998},
  volume    = {1},
}

@InProceedings{histBackObjectTracking,
  author    = {J. Vergés-llahí and J. Ar and A. Sanfeliu},
  title     = {Object tracking system using colour histograms},
  booktitle = {9th Spanish Sym. Pattern Recog. Image Anal},
  year      = {2001},
  pages     = {225--230},
}

@InProceedings{histBackImageIndexing,
  author    = {Swain, Michael J. and Ballard, Dana H.},
  title     = {Indexing via Color Histograms},
  booktitle = {Active Perception and Robot Vision},
  year      = {1992},
  editor    = {Sood, Arun K. and Wechsler, Harry},
  pages     = {261--273},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {The color spectrum of multicolored objects provides a a robust, efficient cue for indexing into a large database of models. This paper shows color histograms to be stable object representations over change in view, and demonstrates they can differentiate among a large number of objects. It introduces a technique called Histogram Intersection for matching model and image histograms and a fast incremental version of Histogram Intersection that allows real-time indexing into a large database of stored models using standard vision hardware. Color can also be used to search for the location of an object. An algorithm called Histogram, Backprojection performs this task efficiently in crowded scenes.},
  isbn      = {978-3-642-77225-2},
}

@InProceedings{histBackMultiObjectTrack,
  author    = {{Jung-ho Lee} and {Woong-hee Lee} and {Dong-seok Jeong}},
  title     = {Object tracking method using back-projection of multiple color histogram models},
  booktitle = {2003 IEEE International Symposium on Circuits and Systems (ISCAS)},
  year      = {2003},
  volume    = {2},
  pages     = {II-II},
  month     = {May},
  doi       = {10.1109/ISCAS.2003.1206062},
  keywords  = {object detection;image representation;optical tracking;video signal processing;object tracking method;back-projection;multiple color histogram models;unmanned observing;representative models;histogram blobs;back-projection image;miss tracking;complex video scenes;Histograms;Layout;Labeling;Gray-scale;Equations;Multimedia systems;Cameras;Color;Games;Robustness},
}

@InProceedings{histBackObjectMultiLighting,
  author    = {{Tse Min Chen} and R. C. {Luo} and {Tsu Hung Hsiao}},
  title     = {Visual tracking using adaptive color histogram model},
  booktitle = {IECON'99. Conference Proceedings. 25th Annual Conference of the IEEE Industrial Electronics Society (Cat. No.99CH37029)},
  year      = {1999},
  volume    = {3},
  pages     = {1336-1341 vol.3},
  month     = {Nov},
  doi       = {10.1109/IECON.1999.819405},
  keywords  = {image colour analysis;computer vision;object recognition;probability;target tracking;visual tracking;adaptive color histogram model;image analysis;object recognition;automation applications;robotics;color based target recognition systems;modeling method;probability distribution;illumination condition change;on-line model parameters adjustment;adaptive color vision system;histogram backprojection algorithm;color object tracking;natural environment;adaptive robustness system;flexible system;Histograms;Lighting;Image color analysis;Machine vision;Target tracking;Object recognition;Robotics and automation;Target recognition;Probability distribution;Adaptive systems},
}

@InProceedings{histBackObjectOfInterestDetection,
  author    = {D. {Gossow} and J. {Pellenz} and D. {Paulus}},
  title     = {Danger Sign Detection Using Color Histograms and SURF Matching},
  booktitle = {2008 IEEE International Workshop on Safety, Security and Rescue Robotics},
  year      = {2008},
  pages     = {13-18},
  month     = {Oct},
  doi       = {10.1109/SSRR.2008.4745870},
  keywords  = {cameras;feature extraction;image colour analysis;image enhancement;image matching;image recognition;image resolution;mobile robots;robot vision;service robots;danger sign detection;color histograms;SURF matching;autonomous rescue robots;map generation;autonomous detection;camera images;exploration algorithm;human rescue teams;speeded up robust feature matching;feature extraction;database samples;affine transformation;object matching;image pixels;Histograms;Pixel;Image recognition;Robot vision systems;Cameras;Humans;Robustness;Feature extraction;Image databases;Spatial databases;Robotic;RoboCup Rescue;danger sign detection;hazmat sign detection},
}

@InProceedings{histBackThermal,
  author = {Bayerl, Felix and Luettel, Thorsten and Wuensche, Hans-Joachim},
  title  = {Following Dirt Roads at Night-Time: Sensors and Features for Lane Recognition and Tracking},
  year   = {2015},
  month  = {09},
}

@Article{histBackRefineShadows,
  author   = {Bao, Jining and Zhang, Yunzhou and Su, Xiaolin and Zheng, Rui},
  title    = {Unpaved road detection based on spatial fuzzy clustering algorithm},
  journal  = {EURASIP Journal on Image and Video Processing},
  year     = {2018},
  volume   = {2018},
  number   = {1},
  pages    = {26},
  month    = {Apr},
  issn     = {1687-5281},
  abstract = {Vision-based unpaved road detection is a challenging task due to the complex nature scene. In this paper, a novel algorithm is proposed to improve the accuracy and robustness of unpaved road detection and boundary extraction with low computational costs. The novelties of this paper are as follows: (1) We use a normal distribution with infrared images to detect the vanishing line, and a trapezoid prediction model is proposed according to the road shape features. (2) Road recognition based on connected regions is implemented by an improved support vector machine (SVM) classifier with a normalized class feature vector. According to the recognition results, the road probability confidence map is obtained. (3) With the help of fusing continuous information with the trapezoidal forecasting model and the probability from the confidence map, we present a road probability recognition method based on the trapezoidal forecasting model and spatial fuzzy clustering. Furthermore, the histogram backprojection model is used to solve interference problems caused by shadows on the road. It takes approximately 0.012{\textasciitilde}0.014 s to process one frame of an image for the road recognition, and the accuracy rate can reach 93.2{\%}. The experimental results show that the algorithm can achieve better performance than some state-of-the-art methods in terms of detection accuracy and speed.},
  day      = {20},
  doi      = {10.1186/s13640-018-0260-3},
  url      = {https://doi.org/10.1186/s13640-018-0260-3},
}

@InBook{explainableCNNBookChapter,
  pages     = {173--193},
  title     = {Explainable Deep Driving by Visualizing Causal Attention},
  publisher = {Springer International Publishing},
  year      = {2018},
  author    = {Kim, Jinkyu and Canny, John},
  editor    = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Bar{\'o}, Xavier and G{\"u}{\c{c}}l{\"u}t{\"u}rk, Ya{\u{g}}mur and G{\"u}{\c{c}}l{\"u}, Umut and van Gerven, Marcel},
  address   = {Cham},
  isbn      = {978-3-319-98131-4},
  abstract  = {Deep neural perception and control networks are likely to be a key component of self-driving vehicles. These models need to be explainable---they should provide easy-to-interpret rationales for their behavior---so that passengers, insurance companies, law enforcement, developers etc., can understand what triggered a particular behavior. Here, we explore the use of visual explanations. These explanations take the form of real-time highlighted regions of an image that causally influence the network's output (steering control). Our approach is two-stage. In the first stage, we use a visual attention model to train a convolutional network end-to-end from images to steering angle. The attention model highlights image regions that potentially influence the network's output. Some of these are true influences, but some are spurious. We then apply a causal filtering step to determine which input regions actually influence the output. This produces more succinct visual explanations and more accurately exposes the network's behavior. We demonstrate the effectiveness of our model on three datasets totaling 16 h of driving. We first show that training with attention does not degrade the performance of the end-to-end network. Then we show that the network highlights interpretable features that are used by humans while driving, and causal filtering achieves a useful reduction in explanation complexity by removing features which do not significantly affect the output.},
  booktitle = {Explainable and Interpretable Models in Computer Vision and Machine Learning},
  doi       = {10.1007/978-3-319-98131-4_8},
}

@InProceedings{opticalFlowSolution,
  author    = {Farneb{\"a}ck, Gunnar},
  title     = {Two-Frame Motion Estimation Based on Polynomial Expansion},
  booktitle = {Image Analysis},
  year      = {2003},
  editor    = {Bigun, Josef and Gustavsson, Tomas},
  pages     = {363--370},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {This paper presents a novel two-frame motion estimation algorithm. The first step is to approximate each neighborhood of both frames by quadratic polynomials, which can be done efficiently using the polynomial expansion transform. From observing how an exact polynomial transforms under translation a method to estimate displacement fields from the polynomial expansion coefficients is derived and after a series of refinements leads to a robust algorithm. Evaluation on the Yosemite sequence shows good results.},
  isbn      = {978-3-540-45103-7},
}

@Article{opticalFlowLKPyramidal,
  author  = {Bouguet, Jean-Yves},
  title   = {Pyramidal Implementation of the Lucas Kanade Feature Tracker Description of the algorithm},
  journal = {OpenCV Document, Intel, Microprocessor Research Labs},
  year    = {2000},
  volume  = {1},
  month   = {01},
}

@InProceedings{opticalFlowLKvsDenseUAV,
  author = {Jasper de Boer and Mathieu Kalksma},
  title  = {Choosing between optical flow algorithms for UAV position change measurement},
  year   = {2015},
}

@Article{keyTechSelfDriving,
  author   = {Jianfeng Zhao and Bodong Liang and Qiuxia Chen},
  title    = {The key technology toward the self-driving car},
  journal  = {International Journal of Intelligent Unmanned Systems},
  year     = {2018},
  volume   = {6},
  number   = {1},
  pages    = {2-20},
  abstract = { Purpose The successful and commercial use of self-driving/driverless/unmanned/automated car will make human life easier. The paper aims to discuss this issue. Design/methodology/approach This paper reviews the key technology of a self-driving car. In this paper, the four key technologies in self-driving car, namely, car navigation system, path planning, environment perception and car control, are addressed and surveyed. The main research institutions and groups in different countries are summarized. Finally, the debates of self-driving car are discussed and the development trend of self-driving car is predicted. Findings This paper analyzes the key technology of self-driving car and illuminates the state-of-art of the self-driving car. Originality/value The main research contents and key technology have been introduced. The research progress as well as the research institution has been summarized. },
  doi      = {10.1108/IJIUS-08-2017-0008},
  eprint   = {https://doi.org/10.1108/IJIUS-08-2017-0008},
  url      = { 
        https://doi.org/10.1108/IJIUS-08-2017-0008
    
},
}

@InProceedings{mitLocalNavDriving,
  author = {Ort, Teddy and Paull, Liam and Rus, Daniela},
  title  = {Autonomous Vehicle Navigation in Rural Environments Without Detailed Prior Maps},
  year   = {2018},
  pages  = {2040-2047},
  month  = {05},
  doi    = {10.1109/ICRA.2018.8460519},
}

@InProceedings{gpsInsFusion,
  author    = {{Qingmei Yang} and {Jianmin Sun}},
  title     = {A location method for autonomous vehicle based on integrated GPS/INS},
  booktitle = {2007 IEEE International Conference on Vehicular Electronics and Safety},
  year      = {2007},
  pages     = {1-4},
  month     = {Dec},
  doi       = {10.1109/ICVES.2007.4456376},
  keywords  = {Global Positioning System;inertial navigation;Kalman filters;remotely operated vehicles;autonomous vehicle;integrated GPS/INS;data fusion;GPS/INS location system;Global Positioning System;inertial navigation system;dead reckoning system;vector electronic map;Kalman filter;Remotely operated vehicles;Mobile robots;Global Positioning System;Sensor systems;Bayesian methods;Probability;Sun;Noise measurement;Inertial navigation;Dead reckoning},
}

@InProceedings{lowCostSensorNav,
  author    = {Rafael Peixoto Derenzi Vivacqua and Raquel Frizera Vassallo and Felipe Nascimento Martins},
  title     = {A Low Cost Sensors Approach for Accurate Vehicle Localization and Autonomous Driving Application},
  booktitle = {Sensors},
  year      = {2017},
}

@Article{robotLIDARSLAM,
  author  = {Bosse, Michael and Zlot, Robert},
  title   = {Map Matching and Data Association for Large-Scale Two-dimensional Laser Scan-based SLAM},
  journal = {I. J. Robotic Res.},
  year    = {2008},
  volume  = {27},
  pages   = {667-691},
  month   = {06},
  doi     = {10.1177/0278364908091366},
}

@InProceedings{canneyAndHoughLanes,
  author    = {A. A. {Assidiq} and O. O. {Khalifa} and M. R. {Islam} and S. {Khan}},
  title     = {Real time lane detection for autonomous vehicles},
  booktitle = {2008 International Conference on Computer and Communication Engineering},
  year      = {2008},
  pages     = {82-88},
  month     = {May},
  doi       = {10.1109/ICCCE.2008.4580573},
  keywords  = {automated highways;computer vision;edge detection;feature extraction;Hough transforms;road safety;road vehicles;traffic engineering computing;real time lane detection;autonomous vehicles;road safety;road accidents;driver assistance system;road lane detection;road boundary detection;road localization;road vehicle heading direction;vision-based lane detection;lighting change;camera;hyperbola;lane edge fitting;lane extraction;Hough transform;curved road;straight road;Vehicle detection;Remotely operated vehicles;Mobile robots;Road vehicles;Road safety;Robustness;Road accidents;Machine vision;Cameras;Testing;Driver Assistance System;Lane detection;computer vision;intelligent vehicles},
}

@Comment{jabref-meta: databaseType:bibtex;}
